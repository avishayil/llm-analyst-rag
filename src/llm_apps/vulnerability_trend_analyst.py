import json
import logging
from typing import Any, Dict, List, Optional, cast

import gradio as gr
import pandas as pd
from fastapi import APIRouter, Depends, FastAPI, Request
from langchain.output_parsers import XMLOutputParser
from langchain_core.outputs.generation import Generation
from slowapi import Limiter
from slowapi.util import get_remote_address

from src.auth_utils.oauth_utils import get_cognito_user, get_user
from src.cve_utils.cve_manager import CVEManager
from src.llm_utils.aws_utils import check_token_limit, update_user_tokens
from src.llm_utils.cache import Cache
from src.llm_utils.constants import BEDROCK_EMBEDDINGS_MODEL_ID
from src.llm_utils.language_model_client import LanguageModelClient
from src.models.api_models import AnalysisRequest
from src.prompts.vulnerability_trend_analysis_prompt import (
    VulnerabilityTrendAnalysisPrompt,
)

logger = logging.getLogger(__name__)

limiter = Limiter(key_func=get_remote_address)


class VulnerabilityTrendAnalyst:
    def __init__(self, app: FastAPI) -> None:
        self.app_name = "vulnerability-analyst"
        self.llm_client = LanguageModelClient()
        self.app_router = APIRouter(prefix=f"/api/v1/{self.app_name}")
        self.setup_gradio_ui(app=app)
        self.setup_routes(app=app)

        # Initialize cache based on the configured model source
        self.cache = Cache(
            embeddings_model=self.llm_client.initialize_bedrock_embeddings_model()
        )

    def inference(
        self,
        criteria: str,
        user: Optional[Dict[str, Any]] = None,
    ) -> pd.DataFrame:
        try:
            logger.info(f"Generating analysis for criteria: {criteria}")

            # Validate input
            request_data = self.validate_input(
                criteria,
            )

            # Generate cache key according to the criteria
            cache_key = criteria

            # Step 1: Check cache for existing llm responses based on the criteria
            cached_response = self.query_cache(cache_key)

            if user and not cached_response:
                # Step 2: No cache hit, generate new response
                new_response = self.generate_new_response(request_data, user["sub"])
                self.cache.update(
                    cache_key=cache_key,
                    llm_string=BEDROCK_EMBEDDINGS_MODEL_ID,
                    value=new_response,
                )
                cached_response = self.query_cache(cache_key)

            # Step 3: Process and return cached response
            return self.process_cached_response(cached_response)

        except Exception as e:
            logger.error(f"Error generating analysis: {e}", exc_info=True)
            return pd.DataFrame({"Error": [str(e)]})

    def validate_input(
        self,
        criteria: str,
    ) -> AnalysisRequest:
        """Validate input for analysis generation."""
        logger.debug(f"Validating input for criteria: {criteria}")
        return AnalysisRequest(criteria=criteria)

    def query_cache(self, cache_key: str) -> List[Generation]:
        """Query the cache for analysis."""
        try:
            cached_response = self.cache.lookup(
                cache_key=cache_key, llm_string=BEDROCK_EMBEDDINGS_MODEL_ID
            )
            return cast(List[Generation], cached_response)
        except Exception as e:
            logger.error(f"Error querying cache: {e}", exc_info=True)
            raise

    def generate_new_response(self, request_data: AnalysisRequest, user_id: str) -> str:
        """Generate new analysis using the language model."""
        logger.info(f"Generating new analysis for criteria: {request_data.criteria}")
        try:
            llm = self.llm_client.initialize_bedrock_model()
            check_token_limit(user_id=user_id)
            cve_manager = CVEManager(
                embeddings_model=self.llm_client.initialize_bedrock_embeddings_model(),
                cve_csv_file_path="data/allitems_clean.csv",
            )
            retriever = cve_manager.get_cve_data_retriever()
            parser = XMLOutputParser()
            prompt = VulnerabilityTrendAnalysisPrompt().prompt_builder(
                criteria=request_data.criteria,
            )
            result = self.llm_client.invoke_chain(
                llm=llm, prompt=prompt, parser=parser, retriever=retriever
            )
            if result:
                update_user_tokens(
                    user_id=user_id, new_tokens_used=result["used_tokens"]
                )
                return json.dumps(result["model_response"])
            else:
                raise
        except Exception as e:
            logger.error(f"Error generating new analysis: {e}", exc_info=True)
            raise

    def process_cached_response(
        self, cached_response: List[Generation]
    ) -> pd.DataFrame:
        """Process the cached response into a DataFrame."""
        logger.info("Processing cached response.")

        # Transform the JSON into a structured format for DataFrame
        structured_data = [
            {
                "Summary": item["trend"][0]["summary"],
            }
            for item in json.loads(cached_response[0].text)
        ]

        try:
            return pd.DataFrame(structured_data)
        except Exception as e:
            logger.error(f"Error processing cached response: {e}", exc_info=True)
            return pd.DataFrame({"Error": [str(e)]})

    def run_inference(
        self,
        criteria: str,
        request: gr.Request,
    ) -> pd.DataFrame:
        user = get_user(request)
        return self.inference(
            criteria=criteria,
            user=user,
        )

    def setup_gradio_ui(self, app: FastAPI) -> None:

        css = """
        #centered-logo {
            display: block;
            margin-left: auto;
            margin-right: auto;
        }

        #centered-text {
            display: block;
            margin-left: auto;
            margin-right: auto;
            width: auto;
        }
        """

        with gr.Blocks(css=css) as iface:
            gr.Image(
                "./src/static/images/logo.png",
                height=200,
                width=200,
                format="png",
                container=False,
                show_download_button=False,
                show_fullscreen_button=False,
                show_label=False,
                show_share_button=False,
                elem_id="centered-logo",
            )
            gr.Markdown("# Vulnerability Analyst", elem_id="centered-text")
            gr.Markdown("## Overview")
            gr.Markdown(
                "This project integrates **FastAPI**, **AWS Cognito**, **Gradio**, and **AWS Bedrock** to provide a user interface and API for interacting with a Large Language Model (LLM) deployed on AWS Bedrock platform."
            )
            gr.Markdown("### Purpose")
            gr.Markdown(
                "The purpose of this application is to demonstrate how modern AI and cloud technologies can be combined to create a user-friendly and intelligent AI web interface and REST API."
            )
            gr.Markdown(
                "The specific use case for this project is a **Vulnerability Analyst**, which assists users in analysing trends in CVEs over time, using historical information from CVE database."
                " By leveraging an LLM hosted on AWS Bedrock, the application can generate insights that help learning about past vulnerabilities in an interesting manner."
            )

            criteria_input = gr.Textbox(
                label="Criteria",
                placeholder="e.g., product, vendor, category, or severity",
            )

            run_button = gr.Button("Generate Analysis")
            output = gr.Dataframe(
                col_count=1, wrap=True, line_breaks=True, interactive=False
            )

            run_button.click(
                fn=self.run_inference,
                inputs=[
                    criteria_input,
                ],
                outputs=output,
            )

        gr.mount_gradio_app(
            app,
            iface,
            path=f"/{self.app_name}",
            auth_dependency=get_user,
        )

    def setup_routes(self, app: FastAPI) -> None:
        @self.app_router.post(path="/generate")
        @limiter.limit(limit_value="10/minute")
        async def protected_route(
            request: Request,
            analysis_request: AnalysisRequest,
            user: dict = Depends(get_cognito_user),  # noqa: B008
        ):
            try:
                result = self.inference(
                    criteria=analysis_request.criteria,
                    user=user,
                )
                return result
            except Exception as e:
                logger.error(f"Error in route: {e}", exc_info=True)
                return {"error": str(e)}

        app.include_router(self.app_router)
